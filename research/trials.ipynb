{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q & A over the code bsea to understand how its works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first it will extract all the code from the link and create a knowledge base based in the code space if what we \n",
    "#### have done and gives the explanation of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo   # for this we take the link (GitPython) to clone the repository from the github\n",
    "from langchain.text_splitter import Language   # language detection\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import  RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder\n",
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this test_repo folder we are gonna clone the repository\n",
    "\n",
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/kuldeep1909/Ent-to-End-Medical-Chatbot-Genai\", to_path=repo_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the code from the repo\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                       glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser=LanguageParser(language= Language.PYTHON, parser_threshold=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, request, jsonify\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_groq import ChatGroq\\nfrom langchain.vectorstores import Pinecone\\nimport pinecone\\n\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import  ChatPromptTemplate\\nfrom dotenv import load_dotenv\\n\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nGROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\\n\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\nos.environ[\\'GROQ_API_KEY\\'] = GROQ_API_KEY\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n#####################################################################\\nindex_name = \"medibot\"\\n\\ndocsearch = Pinecone.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings,\\n)\\n\\nretriever = docsearch.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\": 3})\\n\\nllm = ChatGroq(\\n    temperature = 0,\\n    max_tokens = 500,\\n    groq_api_key = GROQ_API_KEY,\\n    model_name = \"llama-3.3-70b-specdec\"\\n)\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain) \\n\\n\\n\\n\\n## create two route one is deafult for user userinterface for user\\n@app.route(\\'/\\')\\ndef index():\\n    return render_template(\\'index.html\\')\\n\\n## second route for the chat operation for user input\\n@app.route(\\'/get\\', methods=[\\'GET\\', \\'POST\\'])\\ndef chat():\\n    msg = request.form[\\'msg\\']\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\" : msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host = \"0.0.0.0\", port=8080, debug=True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import setup, find_packages\\n\\nsetup(\\n    name = \"GenAI Project\",\\n    version = \"0.0.1\",\\n    author = \"Kuldeep Malviya\",\\n    author_email = \"malviyakuldeep38@gmail.com\",\\n    packages = find_packages(),\\n    install_requires = []\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='# here we will create the entire project folder structure\\nimport os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor filename in list_of_files:\\n    filepath = Path(filename)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Created directory: {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filename}\")\\n\\n    else:\\n        logging.info(f\"File: {filename} already exists\")\\n\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='# here we add all the utility related function\\n\\nfrom langchain.document_loaders import DirectoryLoader, PyPDFLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nimport sentence_transformers\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n## extract the data from the pdf\\n\\ndef load_pdf_data(data):\\n    loader = DirectoryLoader(data, glob=\"*.pdf\",\\n                             loader_cls=PyPDFLoader)\\n    \\n    documents = loader.load()\\n    return documents\\n\\n\\n# perfrom the text splitting \\n\\ndef text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\\n    text_chunk = text_splitter.split_documents(extracted_data)\\n    return text_chunk\\n\\n\\n## download the embedding model from the huggingface\\n\\ndef download_hugging_face_embeddings():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='\\n\\nsystem_prompt = (\"You are an expert medical assistant with comprehensive knowledge across all fields of medicine. \"\\n                \"Use the following pieces of retrieved context to answer.\"\\n                \"the question if you don\\'t know the answer say that you\"\\n                \"don\\'t know use three sentences maximum and keep the\"\\n                \"answer concise.\"\\n\\n                \"\\\\n\\\\n\"\\n                \"{context}\"\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# we need to execute this for the first time \\n\\n# we wanna add some more information then we can add it here by using this code\\n\\nfrom src.helper import load_pdf_data, text_split, download_hugging_face_embeddings\\nimport pinecone\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_community.vectorstores import Pinecone\\nfrom pinecone.grpc import PineconeGRPC as Pinecone \\nfrom pinecone import ServerlessSpec\\n\\n\\nload_dotenv()\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\n\\nextractred_data = load_pdf_data(data=\"Data/\")\\ntext_chunk = text_split(extractred_data)\\nembeddings = download_hugging_face_embeddings()\\n\\n## We have to intialise the pinceone with the python code\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n# Create a serverless index\\nindex_name = \"medibot\"\\n\\nif not pc.has_index(index_name):\\n    pc.create_index(\\n        name=index_name,\\n        dimension=384,\\n        metric=\"cosine\",\\n        spec=ServerlessSpec(\\n            cloud=\\'aws\\', \\n            region=\\'us-east-1\\'\\n        ) \\n    ) \\n\\n# adding the chunks/data\\ndocsearch = Pinecone.from_documents(\\n    documents=text_chunk,\\n    index_name=index_name,\\n    embedding=embeddings,\\n)\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, request, jsonify\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_groq import ChatGroq\\nfrom langchain.vectorstores import Pinecone\\nimport pinecone\\n\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import  ChatPromptTemplate\\nfrom dotenv import load_dotenv\\n\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nGROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\\n\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\nos.environ[\\'GROQ_API_KEY\\'] = GROQ_API_KEY\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n#####################################################################\\nindex_name = \"medibot\"\\n\\ndocsearch = Pinecone.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings,\\n)\\n\\nretriever = docsearch.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\": 3})\\n\\nllm = ChatGroq(\\n    temperature = 0,\\n    max_tokens = 500,\\n    groq_api_key = GROQ_API_KEY,\\n    model_name = \"llama-3.3-70b-specdec\"\\n)\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain) \\n\\n\\n\\n\\n## create two route one is deafult for user userinterface for user\\n@app.route(\\'/\\')\\ndef index():\\n    return render_template(\\'index.html\\')\\n\\n## second route for the chat operation for user input\\n@app.route(\\'/get\\', methods=[\\'GET\\', \\'POST\\'])\\ndef chat():\\n    msg = request.form[\\'msg\\']\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\" : msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host = \"0.0.0.0\", port=8080, debug=True)')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context aware splittig of the code\n",
    "\n",
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language= Language.PYTHON,\n",
    "                                                                  chunk_size= 500,\n",
    "                                                                  chunk_overlap= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, request, jsonify\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_groq import ChatGroq\\nfrom langchain.vectorstores import Pinecone\\nimport pinecone\\n\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import  ChatPromptTemplate\\nfrom dotenv import load_dotenv\\n\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nGROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\\n\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\nos.environ[\\'GROQ_API_KEY\\'] = GROQ_API_KEY\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n#####################################################################\\nindex_name = \"medibot\"\\n\\ndocsearch = Pinecone.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings,\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='retriever = docsearch.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\": 3})\\n\\nllm = ChatGroq(\\n    temperature = 0,\\n    max_tokens = 500,\\n    groq_api_key = GROQ_API_KEY,\\n    model_name = \"llama-3.3-70b-specdec\"\\n)\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"## create two route one is deafult for user userinterface for user\\n@app.route('/')\"),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'index.html\\')\\n\\n## second route for the chat operation for user input\\n@app.route(\\'/get\\', methods=[\\'GET\\', \\'POST\\'])\\ndef chat():\\n    msg = request.form[\\'msg\\']\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\" : msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host = \"0.0.0.0\", port=8080, debug=True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import setup, find_packages\\n\\nsetup(\\n    name = \"GenAI Project\",\\n    version = \"0.0.1\",\\n    author = \"Kuldeep Malviya\",\\n    author_email = \"malviyakuldeep38@gmail.com\",\\n    packages = find_packages(),\\n    install_requires = []\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='# here we will create the entire project folder structure\\nimport os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor filename in list_of_files:\\n    filepath = Path(filename)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Created directory: {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filename}\")\\n\\n    else:\\n        logging.info(f\"File: {filename} already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='# here we add all the utility related function\\n\\nfrom langchain.document_loaders import DirectoryLoader, PyPDFLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nimport sentence_transformers\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n\\n\\n## extract the data from the pdf'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def load_pdf_data(data):\\n    loader = DirectoryLoader(data, glob=\"*.pdf\",\\n                             loader_cls=PyPDFLoader)\\n    \\n    documents = loader.load()\\n    return documents\\n\\n\\n# perfrom the text splitting \\n\\ndef text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\\n    text_chunk = text_splitter.split_documents(extracted_data)\\n    return text_chunk\\n\\n\\n## download the embedding model from the huggingface'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def download_hugging_face_embeddings():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = (\"You are an expert medical assistant with comprehensive knowledge across all fields of medicine. \"\\n                \"Use the following pieces of retrieved context to answer.\"\\n                \"the question if you don\\'t know the answer say that you\"\\n                \"don\\'t know use three sentences maximum and keep the\"\\n                \"answer concise.\"\\n\\n                \"\\\\n\\\\n\"\\n                \"{context}\"\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='# we need to execute this for the first time \\n\\n# we wanna add some more information then we can add it here by using this code\\n\\nfrom src.helper import load_pdf_data, text_split, download_hugging_face_embeddings\\nimport pinecone\\nimport os\\nfrom dotenv import load_dotenv\\nfrom langchain_community.vectorstores import Pinecone\\nfrom pinecone.grpc import PineconeGRPC as Pinecone \\nfrom pinecone import ServerlessSpec'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='load_dotenv()\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nos.environ[\\'PINECONE_API_KEY\\'] = PINECONE_API_KEY\\n\\nextractred_data = load_pdf_data(data=\"Data/\")\\ntext_chunk = text_split(extractred_data)\\nembeddings = download_hugging_face_embeddings()\\n\\n## We have to intialise the pinceone with the python code\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n# Create a serverless index\\nindex_name = \"medibot\"'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='if not pc.has_index(index_name):\\n    pc.create_index(\\n        name=index_name,\\n        dimension=384,\\n        metric=\"cosine\",\\n        spec=ServerlessSpec(\\n            cloud=\\'aws\\', \\n            region=\\'us-east-1\\'\\n        ) \\n    ) \\n\\n# adding the chunks/data\\ndocsearch = Pinecone.from_documents(\\n    documents=text_chunk,\\n    index_name=index_name,\\n    embedding=embeddings,\\n)')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malvi\\anaconda3\\envs\\sourcecode\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No sentence-transformers model found with name microsoft/codebert-base. Creating a new one with mean pooling.\n",
      "c:\\Users\\malvi\\anaconda3\\envs\\sourcecode\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\malvi\\.cache\\huggingface\\hub\\models--microsoft--codebert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# we need to conver the code into embeddings\n",
    "\n",
    "from langchain.embeddings import  HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "test = embeddings.embed_query(\"Hello Kuldeep\")\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.14 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.19 which is incompatible.\n",
      "langchain-core 0.3.29 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.19 which is incompatible.\n",
      "langchain-groq 0.1.10 requires langchain-core<0.3.0,>=0.2.39, but you have langchain-core 0.3.29 which is incompatible.\n",
      "pydantic-settings 2.7.1 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip -q install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(texts, embeddings, persist_directory= './db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malvi\\AppData\\Local\\Temp\\ipykernel_16552\\1303152908.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# to activate this vectorstore\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic<2.0.0\n",
      "  Using cached pydantic-1.10.19-cp310-cp310-win_amd64.whl.metadata (153 kB)\n",
      "Collecting typing-extensions>=4.2.0 (from pydantic<2.0.0)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Using cached pydantic-1.10.19-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, pydantic\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.4\n",
      "    Uninstalling pydantic-2.10.4:\n",
      "      Successfully uninstalled pydantic-2.10.4\n",
      "Successfully installed pydantic-1.10.19 typing-extensions-4.12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.14 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.19 which is incompatible.\n",
      "langchain-core 0.3.29 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.19 which is incompatible.\n",
      "langchain-groq 0.1.10 requires langchain-core<0.3.0,>=0.2.39, but you have langchain-core 0.3.29 which is incompatible.\n",
      "pydantic-settings 2.7.1 requires pydantic>=2.7.0, but you have pydantic 1.10.19 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install \"pydantic<2.0.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Using cached langchain_groq-0.2.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
      "  Using cached groq-0.13.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain-groq)\n",
      "  Using cached langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from groq<1,>=0.4.1->langchain-groq)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting sniffio (from groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing-extensions<5,>=4.10 (from groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.3,>=0.1.125 (from langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached langsmith-0.2.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached orjson-3.10.13-cp310-cp310-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests<3,>=2 (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq)\n",
      "  Using cached pydantic_core-2.27.2-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-groq)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Using cached langchain_groq-0.2.2-py3-none-any.whl (14 kB)\n",
      "Using cached groq-0.13.1-py3-none-any.whl (109 kB)\n",
      "Using cached langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.2.10-py3-none-any.whl (326 kB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached orjson-3.10.13-cp310-cp310-win_amd64.whl (135 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tenacity, sniffio, PyYAML, packaging, orjson, jsonpointer, idna, h11, exceptiongroup, distro, charset-normalizer, certifi, annotated-types, requests, pydantic-core, jsonpatch, httpcore, anyio, requests-toolbelt, pydantic, httpx, langsmith, groq, langchain-core, langchain-groq\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.5.0\n",
      "    Uninstalling tenacity-8.5.0:\n",
      "      Successfully uninstalled tenacity-8.5.0\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: orjson\n",
      "    Found existing installation: orjson 3.10.13\n",
      "    Uninstalling orjson-3.10.13:\n",
      "      Successfully uninstalled orjson-3.10.13\n",
      "  Attempting uninstall: jsonpointer\n",
      "    Found existing installation: jsonpointer 3.0.0\n",
      "    Uninstalling jsonpointer-3.0.0:\n",
      "      Successfully uninstalled jsonpointer-3.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.2.2\n",
      "    Uninstalling exceptiongroup-1.2.2:\n",
      "      Successfully uninstalled exceptiongroup-1.2.2\n",
      "  Attempting uninstall: distro\n",
      "    Found existing installation: distro 1.9.0\n",
      "    Uninstalling distro-1.9.0:\n",
      "      Successfully uninstalled distro-1.9.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.1\n",
      "    Uninstalling charset-normalizer-3.4.1:\n",
      "      Successfully uninstalled charset-normalizer-3.4.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.12.14\n",
      "    Uninstalling certifi-2024.12.14:\n",
      "      Successfully uninstalled certifi-2024.12.14\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.7.0\n",
      "    Uninstalling annotated-types-0.7.0:\n",
      "      Successfully uninstalled annotated-types-0.7.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.27.2\n",
      "    Uninstalling pydantic_core-2.27.2:\n",
      "      Successfully uninstalled pydantic_core-2.27.2\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.33\n",
      "    Uninstalling jsonpatch-1.33:\n",
      "      Successfully uninstalled jsonpatch-1.33\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.7\n",
      "    Uninstalling httpcore-1.0.7:\n",
      "      Successfully uninstalled httpcore-1.0.7\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.7.0\n",
      "    Uninstalling anyio-4.7.0:\n",
      "      Successfully uninstalled anyio-4.7.0\n",
      "  Attempting uninstall: requests-toolbelt\n",
      "    Found existing installation: requests-toolbelt 1.0.0\n",
      "    Uninstalling requests-toolbelt-1.0.0:\n",
      "      Successfully uninstalled requests-toolbelt-1.0.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.19\n",
      "    Uninstalling pydantic-1.10.19:\n",
      "      Successfully uninstalled pydantic-1.10.19\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.147\n",
      "    Uninstalling langsmith-0.1.147:\n",
      "      Successfully uninstalled langsmith-0.1.147\n",
      "  Attempting uninstall: groq\n",
      "    Found existing installation: groq 0.13.1\n",
      "    Uninstalling groq-0.13.1:\n",
      "      Successfully uninstalled groq-0.13.1\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.29\n",
      "    Uninstalling langchain-core-0.3.29:\n",
      "      Successfully uninstalled langchain-core-0.3.29\n",
      "  Attempting uninstall: langchain-groq\n",
      "    Found existing installation: langchain-groq 0.1.10\n",
      "    Uninstalling langchain-groq-0.1.10:\n",
      "      Successfully uninstalled langchain-groq-0.1.10\n",
      "Successfully installed PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.8.0 certifi-2024.12.14 charset-normalizer-3.4.1 distro-1.9.0 exceptiongroup-1.2.2 groq-0.13.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.29 langchain-groq-0.2.2 langsmith-0.2.10 orjson-3.10.13 packaging-24.2 pydantic-2.10.4 pydantic-core-2.27.2 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 typing-extensions-4.12.2 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\malvi\\anaconda3\\envs\\sourcecode\\Lib\\site-packages\\~aml'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\malvi\\anaconda3\\envs\\sourcecode\\Lib\\site-packages\\~rjson'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\malvi\\anaconda3\\envs\\sourcecode\\Lib\\site-packages\\~harset_normalizer'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\malvi\\anaconda3\\envs\\sourcecode\\Lib\\site-packages\\~ydantic_core'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the llm from groq\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    groq_api_key = GROQ_API_KEY,\n",
    "    model_name = \"llama-3.3-70b-specdec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malvi\\AppData\\Local\\Temp\\ipykernel_16552\\1764516168.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the final chian\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is download_hugging_face_embeddings function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 15, updating n_results = 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'what is download_hugging_face_embeddings function',\n",
       " 'chat_history': [SystemMessage(content='', additional_kwargs={}, response_metadata={})],\n",
       " 'answer': 'The `download_hugging_face_embeddings` function is used to download and initialize a Hugging Face embeddings model. \\n\\nIn this specific code, it is defined as:\\n\\n```python\\ndef download_hugging_face_embeddings():\\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\\n    return embeddings\\n```\\n\\nThis function uses the `HuggingFaceEmbeddings` class from the `langchain.embeddings` module to create an instance of the \"sentence-transformers/all-MiniLM-L6-v2\" model. This model is a pre-trained language model that can be used for various natural language processing tasks, such as text classification, sentiment analysis, and more.\\n\\nThe `download_hugging_face_embeddings` function returns an instance of the `HuggingFaceEmbeddings` class, which can be used to generate embeddings for input text. \\n\\nIn the context of the provided code, this function is used to generate embeddings for the text extracted from PDF files, which are then used to create a Pinecone index for efficient similarity search and retrieval.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 15, updating n_results = 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'what is load_pdf_data function', 'chat_history': [SystemMessage(content='The human asks about the `download_hugging_face_embeddings` function. The `download_hugging_face_embeddings` function is used to download and initialize a Hugging Face embeddings model, specifically the \"sentence-transformers/all-MiniLM-L6-v2\" model, which is a pre-trained language model for various natural language processing tasks. The function returns an instance of the `HuggingFaceEmbeddings` class, used to generate embeddings for input text, and in this context, is applied to generate embeddings from text extracted from PDF files for creating a Pinecone index. The human also inquires about the RecursiveCharacterTextSplitter function, which is a text splitting utility from the Langchain library that splits documents into smaller chunks based on a specified character limit, and in the provided code, it is used to split the extracted PDF data into chunks of 1000 characters with a 20-character overlap between chunks to prepare the text data for further processing, such as embedding and indexing, using parameters `chunk_size=1000` and `chunk_overlap=20`. The human then asks for more information about the `RecursiveCharacterTextSplitter`, and it is explained that it is a class from the `langchain.text_splitter` module that recursively splits text into smaller chunks based on a specified character, allowing for fine-grained control over the text splitting process, and although it is imported in the provided code, it is not used; instead, a custom `text_split` function from the `src.helper` module is used to split the text, with its implementation not shown in the code snippet.', additional_kwargs={}, response_metadata={})], 'answer': 'The `load_pdf_data` function is not defined in the provided code snippet. However, based on its usage, it appears to be a function that extracts data from PDF files.\\n\\nIt is imported from the `src.helper` module, suggesting that it is a custom function defined elsewhere in the codebase. The function takes a `data` parameter, which is set to `\"Data/\"` in the provided code, implying that it may be loading PDF data from a directory named \"Data\".\\n\\nWithout the actual function definition, the exact behavior and implementation of `load_pdf_data` are unknown.'}\n"
     ]
    }
   ],
   "source": [
    "question = \"what is load_pdf_data function\"\n",
    "print(qa(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sourcecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
